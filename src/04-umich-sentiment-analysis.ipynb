{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Sentiment Analysis\n",
    "\n",
    "We attempt to do some sentiment analysis with a dataset provided by the University of Michigan for the Kaggle [UMICH SI650 - Sentiment Classification](https://inclass.kaggle.com/c/si650winter11) competition. We will only use the training data, since the test data is unlabeled and we cannot run an evaluation locally.\n",
    "\n",
    "## Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen: 42, vocab size: 2313\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "word_freqs = collections.Counter()\n",
    "num_recs = 0\n",
    "ftrain = open(\"../data/umich-sentiment-train.txt\", \"rb\")\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        word_freqs[word] += 1\n",
    "    num_recs += 1\n",
    "ftrain.close()\n",
    "\n",
    "# print some statistics about our data, that will drive our parameters\n",
    "print(\"maxlen: %d, vocab size: %d\" % (maxlen, len(word_freqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 2000\n",
    "MAX_SENTENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# special words: UNK = -1, PAD = 0\n",
    "vocab = {\"UNK\": -1, \"PAD\": 0}\n",
    "reverse_vocab = {v:k for k, v in vocab.items()}\n",
    "for idx, word in enumerate([w[0] for w in word_freqs.most_common(MAX_FEATURES - 1)]):\n",
    "    vocab[word] = idx + 1\n",
    "    reverse_vocab[idx + 1] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sentences to token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.empty((num_recs, ), dtype=list)\n",
    "y = np.zeros((num_recs, ))\n",
    "i = 0\n",
    "ftrain = open(\"../data/umich-sentiment-train.txt\", \"rb\")\n",
    "for line in ftrain:\n",
    "    label, sentence = line.strip().split(\"\\t\")\n",
    "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
    "    seqs = []\n",
    "    for word in words:\n",
    "        seqs.append(vocab.get(word, -1))\n",
    "    X[i] = seqs\n",
    "    y[i] = int(label)\n",
    "    i += 1\n",
    "ftrain.close()\n",
    "\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split input into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4960, 40) (2126, 40) (4960,) (2126,)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "Note that our last layer is a single Dense node, since we want a score for our sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_FEATURES, 128, input_length=MAX_SENTENCE_LENGTH, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4960 samples, validate on 2126 samples\n",
      "Epoch 1/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.3371 - acc: 0.8506 - val_loss: 0.0905 - val_acc: 0.9704\n",
      "Epoch 2/10\n",
      "4960/4960 [==============================] - 12s - loss: 0.0991 - acc: 0.9591 - val_loss: 0.0409 - val_acc: 0.9854\n",
      "Epoch 3/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0851 - acc: 0.9643 - val_loss: 0.0542 - val_acc: 0.9779\n",
      "Epoch 4/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0666 - acc: 0.9734 - val_loss: 0.0428 - val_acc: 0.9849\n",
      "Epoch 5/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0646 - acc: 0.9738 - val_loss: 0.0348 - val_acc: 0.9864\n",
      "Epoch 6/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0505 - acc: 0.9817 - val_loss: 0.0404 - val_acc: 0.9854\n",
      "Epoch 7/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0549 - acc: 0.9756 - val_loss: 0.0352 - val_acc: 0.9854\n",
      "Epoch 8/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0483 - acc: 0.9821 - val_loss: 0.0378 - val_acc: 0.9854\n",
      "Epoch 9/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0499 - acc: 0.9821 - val_loss: 0.0449 - val_acc: 0.9835\n",
      "Epoch 10/10\n",
      "4960/4960 [==============================] - 13s - loss: 0.0475 - acc: 0.9798 - val_loss: 0.0389 - val_acc: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115499390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, batch_size=32, nb_epoch=10, validation_data=(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126/2126 [==============================] - 1s     \n",
      "loss on test set: 0.039, accuracy: 0.988\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(Xtest, ytest, batch_size=32)\n",
    "print(\"loss on test set: %.3f, accuracy: %.3f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment on some random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000\t0\tda vinci code is awesome ! !\n",
      "0.000\t0\tharry potter sucks ! !\n",
      "0.000\t1\tthe da vinci code sucked big time .\n",
      "1.000\t1\tthe da vinci code is awesome ! !\n",
      "0.001\t1\tbrokeback mountain is fucking horrible..\n",
      "1.000\t0\ti love brokeback mountain !\n",
      "1.000\t0\tbecause i would like to make friends who like the same things i like , and i really like harry potter , so i thought that joining a community like this would be a good start .\n",
      "1.000\t1\tthe da vinci code was awesome , i ca n't wait to read it ...\n",
      "0.000\t0\toh , and brokeback mountain was a terrible movie .\n",
      "1.000\t1\tda vinci code is awesome ! !\n"
     ]
    }
   ],
   "source": [
    "random_idxs = np.random.randint(0, Xtest.shape[0], 10)\n",
    "for i in range(random_idxs.shape[0]):\n",
    "    xtest = Xtest[random_idxs[i]].reshape(1, MAX_SENTENCE_LENGTH)\n",
    "    ylabel = ytest[i]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent_pred = \" \".join([reverse_vocab[x] for x in xtest[0].tolist() if x != 0])\n",
    "    print(\"%.3f\\t%d\\t%s\" % (ypred, ylabel, sent_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
